{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677c27b9",
   "metadata": {},
   "source": [
    "Linux foundation Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a data lakehouse architecture in Spark to support SQL_based data manipulation semantics with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf599a3e",
   "metadata": {},
   "source": [
    "### Understanding Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ae54f",
   "metadata": {},
   "source": [
    "Delta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Delta Lake is supported in Azure Synapse Analytics Spark pools for PySpark, Scala, and .NET code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5db8be",
   "metadata": {},
   "source": [
    "### Creating a Delta Lake from a dataframe\n",
    "One of the easiest ways to create a Delta Lake table is to save a dataframe in the delta format, specifying a path where the data files and related metadata information for the table should be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a file into a dataframe\n",
    "df = spark.read.load('/data/mydata.csv', format='csv', header=True)\n",
    "\n",
    "# Save the dataframe as a delta table\n",
    "delta_table_path = \"/delta/mydata\"\n",
    "df.write.format(\"delta\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37221359",
   "metadata": {},
   "source": [
    "### Querying a previous version of a table\n",
    "After saving the delta table, the path location you specified includes parquet files for the data (regardless of the format of the source file you loaded into the dataframe) and a <b>_delta_log folder containing the transaction log for the table</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33f3b8",
   "metadata": {},
   "source": [
    "Delta Lake tables support versioning through the transaction log. The transaction log records modifications made to the table, noting the timestamp and version number for each transaction. You can use this logged version data to view previous versions of the table - a feature known as time travel.\n",
    "\n",
    "You can retrieve data from a specific version of a Delta Lake table by reading the data from the delta table location into a dataframe, specifying the version required as a <b>versionAsOf</b> option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3473dd3",
   "metadata": {},
   "source": [
    "Alternatively, you can specify a timestamp by using the <b>timestampAsOf</b> option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd78460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928cf732",
   "metadata": {},
   "source": [
    "### Use Delta Lake with streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2620d5",
   "metadata": {},
   "source": [
    "Many data analytics scenarios involve streaming data that must be processed in near real time. For example, you might need \n",
    "to capture readings emitted by internet-of-things (IoT) devices and store them in a table as they occur.\n",
    "\n",
    "A typical stream processing solution involves constantly reading a stream of data from a source, optionally processing it to select specific fields, aggregate and group values, or otherwise manipulate the data, and writing the results to a sink.\n",
    "\n",
    "ark includes native support for streaming data through <b>Spark Structured Streaming</b>, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka, or file system locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ac8a0",
   "metadata": {},
   "source": [
    "#### Using Delta Lake table as streaming source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Load a streaming dataframe from the Delta Table\n",
    "stream_df = spark.readStream.format(\"delta\") \\\n",
    "    .option(\"ignoreChanges\", \"true\") \\\n",
    "    .load(\"/delta/internetorders\")\n",
    "\n",
    "# Now you can process the streaming data in the dataframe\n",
    "# for example, show it:\n",
    "stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
