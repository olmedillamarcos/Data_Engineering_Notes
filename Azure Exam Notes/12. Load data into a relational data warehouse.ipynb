{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0e126a",
   "metadata": {},
   "source": [
    "We'll focus on ways that you can use SQL to load data into tables in a dedicated SQL pool in Azure Synapse Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba88c6",
   "metadata": {},
   "source": [
    "## 1. Load Staging Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025fce2",
   "metadata": {},
   "source": [
    "One of the most common patterns for loading a data warehouse is to transfer data from source systems to files in a data lake, ingest the file data into staging tables, and then use SQL statements to load the data from the staging tables into the dimension and fact tables. Usually data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90283ea6",
   "metadata": {},
   "source": [
    "#### Creating Staging Tables\n",
    "Many organized warehouses have standard structures for staging the database and might even use a specific schema for staging the data. The following code example creates a staging table for product data that will ultimately be loaded into a dimension table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE dbo.StageProduct\n",
    "(\n",
    "    ProductID NVARCHAR(10) NOT NULL,\n",
    "    ProductName NVARCHAR(200) NOT NULL,\n",
    "    ProductCategory NVARCHAR(200) NOT NULL,\n",
    "    Color NVARCHAR(10),\n",
    "    Size NVARCHAR(10),\n",
    "    ListPrice DECIMAL NOT NULL,\n",
    "    Discontinued BIT NOT NULL\n",
    ")\n",
    "WITH\n",
    "(\n",
    "    DISTRIBUTION = ROUND_ROBIN,\n",
    "    CLUSTERED COLUMNSTORE INDEX\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351596a5",
   "metadata": {},
   "source": [
    "#### Using the COPY command\n",
    "\n",
    "You can use the COPY statement to load data from the data lake, as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY INTO dbo.StageProduct\n",
    "    (ProductID, ProductName, ...)\n",
    "FROM 'https://mydatalake.../data/products*.parquet'\n",
    "WITH\n",
    "(\n",
    "    FILE_TYPE = 'PARQUET',\n",
    "    MAXERRORS = 0,\n",
    "    IDENTITY_INSERT = 'OFF'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46fc71",
   "metadata": {},
   "source": [
    "### Using External Tables\n",
    "In some cases, if the data to be loaded is stored in files with an appropriate structure, it can be more effective to create external tables that reference the file location. This way, the data can be read directly from the source files instead of being loaded into the relational store. The following example, shows how to create an external table that references files in the data lake associated with the Azure Synapse Analytics workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc068ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE dbo.ExternalStageProduct\n",
    " (\n",
    "     ProductID NVARCHAR(10) NOT NULL,\n",
    "     ProductName NVARCHAR(10) NOT NULL,\n",
    " ...\n",
    " )\n",
    "WITH\n",
    " (\n",
    "    DATE_SOURCE = StagedFiles,\n",
    "    LOCATION = 'folder_name/*.parquet',\n",
    "    FILE_FORMAT = ParquetFormat\n",
    " );\n",
    "GO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590b92f",
   "metadata": {},
   "source": [
    "## 2. Load Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c8468b",
   "metadata": {},
   "source": [
    "After staging dimension data, you can load it into dimension tables using SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ee4a3",
   "metadata": {},
   "source": [
    "### Using a CREATE TABLE AS (CTAS) statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c13f5d",
   "metadata": {},
   "source": [
    "#### Using INSERT\n",
    "When you need to load staged data into an existing dimension table, you can use an INSERT statement. <b>This approach works if the staged data contains only records for new dimension entities (not updates to existing entities)</b>. This approach is much less complicated than the technique in the last section, which required a UNION ALL and then renaming table objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b99d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT INTO dbo.DimCustomer\n",
    "SELECT CustomerNo AS CustAltKey,\n",
    "    CustomerName,\n",
    "    EmailAddress,\n",
    "    Phone,\n",
    "    StreetAddress,\n",
    "    City,\n",
    "    PostalCode,\n",
    "    CountryRegion\n",
    "FROM dbo.StageCustomers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ebc958",
   "metadata": {},
   "source": [
    "### Load Slowly Changing Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b6f2e",
   "metadata": {},
   "source": [
    "#### Types of SCD\n",
    "\n",
    "* Type 0\n",
    " * Type 0 dimension data can't be changed. Any attempted changes fail.\n",
    "* Type 1\n",
    " * In type 1 dimensions, the dimension record is updated in-place. Changes made to an existing dimension row apply to all previously loaded facts related to the dimension.\n",
    "* Type 2\n",
    " * In a type 2 dimension, a change to a dimension results in a new dimension row. Existing rows for previous versions of the dimension are retained for historical fact analysis and the new row is applied to future fact table entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cd922",
   "metadata": {},
   "source": [
    "Logic to implement Type 1 and Type 2 updates can be complex, and there are various techniques you can use. For example, you could use a combination of UPDATE and INSERT statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554817fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- New Customers\n",
    "INSERT INTO dbo.DimCustomer\n",
    "SELECT stg.*\n",
    "FROM dbo.StageCustomers AS stg\n",
    "WHERE NOT EXISTS\n",
    "    (SELECT * FROM dbo.DimCustomer AS dim\n",
    "    WHERE dim.CustomerAltKey = stg.CustNo)\n",
    "\n",
    "-- Type 1 updates (name)\n",
    "UPDATE dbo.DimCustomer\n",
    "SET CustomerName = stg.CustomerName\n",
    "FROM dbo.StageCustomers AS stg\n",
    "WHERE dbo.DimCustomer.CustomerAltKey = stg.CustomerNo;\n",
    "\n",
    "-- Type 2 updates (StreetAddress)\n",
    "INSERT INTO dbo.DimCustomer\n",
    "SELECT stg.*\n",
    "FROM dbo.StageCustomers AS stg\n",
    "JOIN dbo.DimCustomer AS dim\n",
    "ON stg.CustNo = dim.CustomerAltKey\n",
    "AND stg.StreetAddress <> dim.StreetAddress;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf316c54",
   "metadata": {},
   "source": [
    "As an alternative to using multiple INSERT and UPDATE statements, you can use a single MERGE statement to perform an \"upsert\" operation to insert new records and update existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e4cca",
   "metadata": {},
   "source": [
    "## 3. Loading Fact Tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add92d8",
   "metadata": {},
   "source": [
    "Typically, a regular data warehouse load operation loads fact tables after dimension tables. This approach ensures that the dimensions to which the facts will be related are already present in the data warehouse.\n",
    "\n",
    "The staged fact data usually includes the business (alternate) keys for the related dimensions, so your logic to load the data must look up the corresponding surrogate keys. When the data warehouse slowly changing dimensions, the appropriate version of the dimension record must be identified to ensure the correct surrogate key is used to match the event recorded in the fact table with the state of the dimension at the time the fact occurred.\n",
    "\n",
    "In many cases, you can retrieve the latest \"current\" version of the dimension; but in some cases you might need to find the right dimension record based on DateTime columns that indicate the period of validity for each version of the dimension.\n",
    "\n",
    "The following example assumes that the dimension records have an incrementing surrogate key, and that the most recently added version of a specific dimension instance (which will have the highest key value) should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa65593",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT INTO dbo.FactSales\n",
    "SELECT  (SELECT MAX(DateKey)\n",
    "         FROM dbo.DimDate\n",
    "         WHERE FullDateAlternateKey = stg.OrderDate) AS OrderDateKey,\n",
    "        (SELECT MAX(CustomerKey)\n",
    "         FROM dbo.DimCustomer\n",
    "         WHERE CustomerAlternateKey = stg.CustNo) AS CustomerKey,\n",
    "        (SELECT MAX(ProductKey)\n",
    "         FROM dbo.DimProduct\n",
    "         WHERE ProductAlternateKey = stg.ProductID) AS ProductKey,\n",
    "        (SELECT MAX(StoreKey)\n",
    "         FROM dbo.DimStore\n",
    "         WHERE StoreAlternateKey = stg.StoreID) AS StoreKey,\n",
    "        OrderNumber,\n",
    "        OrderLineItem,\n",
    "        OrderQuantity,\n",
    "        UnitPrice,\n",
    "        Discount,\n",
    "        Tax,\n",
    "        SalesAmount\n",
    "FROM dbo.StageSales AS stg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
