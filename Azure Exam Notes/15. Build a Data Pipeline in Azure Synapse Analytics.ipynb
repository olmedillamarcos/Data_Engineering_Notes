{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c155fb",
   "metadata": {},
   "source": [
    "# Core pipeline concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73ac86",
   "metadata": {},
   "source": [
    "#### Activities\n",
    "\n",
    "Activities are the executable tasks in a pipeline. You can define a flow of activities by connecting them in a sequence. The outcome of a particular activity (success, failure, or completion) can be used to direct the flow to the next activity in the sequence.\n",
    "\n",
    "Activities can encapsulate data transfer operations, including simple data copy operations that extract data from a source and load it to a target (or sink), as well as more complex data flows that apply transformations to the data as part of an extract, transfer, and load (ETL) operation. Additionally, there are activities that encapsulate processing tasks on specific systems, such as running a Spark notebook or calling an Azure function. Finally, there are control flow activities that you can use to implement loops, conditional branching, or manage variable and parameter values.\n",
    "\n",
    "#### Integration runtime\n",
    "\n",
    "The pipeline requires compute resources and an execution context in which to run. The pipeline's integration runtime provides this context, and is used to initiate and coordinate the activities in the pipeline.\n",
    "\n",
    "#### Linked services\n",
    "\n",
    "While many of the activities are run directly in the integration runtime for the pipeline, some activities depend on external services. For example, a pipeline might include an activity to run a notebook in Azure Databricks or to call a stored procedure in Azure SQL Database. To enable secure connections to the external services used by your pipelines, you must define linked services for them.\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "Most pipelines process data, and the specific data that is consumed and produced by activities in a pipeline is defined using datasets. A dataset defines the schema for each data object that will be used in the pipeline, and has an associated linked service to connect to its source. Activities can have datasets as inputs or outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4717332b",
   "metadata": {},
   "source": [
    "While developing a data flow, you can enable the <b>Data flow debug</b> option to pass a subset of data through the flow, which can be useful to test that your columns are mapped correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f224584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
