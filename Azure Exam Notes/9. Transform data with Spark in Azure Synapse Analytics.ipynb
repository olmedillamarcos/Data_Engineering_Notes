{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6d0abe",
   "metadata": {},
   "source": [
    "Apache Spark provides a powerful platform for performing data cleansing and transformation tasks on large volumes of data. By using the Spark dataframe object, you can easily load data from files in a data lake and perform complex modifications. You can then save the transformed data back to the data lake for downstream processing or ingestion into a data warehouse.\n",
    "\n",
    "Azure Synapse Analytics provides Apache Spark pools that you can use to run Spark workloads to transform data as part of a data ingestion and preparation workload. You can use natively supported notebooks to write and run code on a Spark pool to prepare data for analysis. You can then use other Azure Synapse Analytics capabilities such as SQL pools to work with the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef270274",
   "metadata": {},
   "outputs": [],
   "source": [
    "####reading\n",
    "order_details = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\n",
    "display(order_details.limit(5))\n",
    "\n",
    "####transforming\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Create the new FirstName and LastName fields\n",
    "transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
    "\n",
    "# Remove the CustomerName field\n",
    "transformed_df = transformed_df.drop(\"CustomerName\")\n",
    "\n",
    "display(transformed_df.limit(5))\n",
    "\n",
    "####saving\n",
    "transformed_df.write.mode(\"overwrite\").parquet('/transformed_data/orders.parquet')\n",
    "print (\"Transformed data saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561e483",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d58099",
   "metadata": {},
   "source": [
    "Partitioning is an optimization technique that enables spark to maximize performance across the worker nodes. More performance gains can be achieved when filtering data in queries by eliminating unnecessary disk IO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "# Load source data\n",
    "df = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Add Year column\n",
    "dated_df = df.withColumn(\"Year\", year(col(\"OrderDate\")))\n",
    "\n",
    "# Partition by year\n",
    "dated_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4c7d0",
   "metadata": {},
   "source": [
    "### Using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0bcf2",
   "metadata": {},
   "source": [
    "The SparkSQL library, which provides the dataframe structure also enables you to use SQL as a way of working with data. With this approach, You can query and transform data in dataframes by using SQL queries, and persist the results as tables.\n",
    "\n",
    "\n",
    "Tables are metadata abstractions over files. The data is not stored in a relational table, but the table provides a relational layer over files in the data lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d304f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
